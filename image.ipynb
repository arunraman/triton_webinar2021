{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: center;\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Triton Server"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Overview\n",
    "\n",
    "This notebook shows the procedure to deploy a Triton Inference Server with image model and compare inferencing on GPU/CPU. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup <a class=\"anchor\" id=\"Setup\"></a>\n",
    "\n",
    "To begin, check that the NVIDIA driver has been installed correctly. The `nvidia-smi` command should run and output information about the GPUs on your system:\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!nvidia-smi"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Start the Triton Server"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets start the triton server in polling mode."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run the Triton Inference Server in a seperate termial from the Jupyter Notebook\n",
    "```\n",
    "tritonserver  --model-repository=/tritonworkspace/models --model-control-mode=POLL\n",
    "````"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above command should load the model from the model directory and print the log `successfully loaded 'inception_grapghdef' version 1`. Triton server listens on the following endpoints:\n",
    "\n",
    "```\n",
    "Port 8000    -> HTTP Service\n",
    "Port 8001    -> GRPC Service\n",
    "Port 8002    -> Metrics\n",
    "```\n",
    "\n",
    "We can test the status of the server connection by running the curl command: `curl -v <IP of machine>:9000/v2/health/ready` which should return `HTTP/1.1 200 OK`\n",
    "\n",
    "**NOTE:-** In our case the IP of machine on which Triton Server and this notebook are currently running is `localhost`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!curl -v localhost:8000/v2/health/ready"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check inference with image_client "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets check the test data from the images folder"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from IPython.display import Image, display\n",
    "listOfImageNames = ['/tritonworkspace/images/basketball.jpg',\n",
    "                    '/tritonworkspace/images/football.jpg',\n",
    "                    '/tritonworkspace/images/soccer_ball.jpg',\n",
    "                    '/tritonworkspace/images/volleyball.jpg']\n",
    "\n",
    "for imageName in listOfImageNames:\n",
    "    display(Image(filename=imageName, width = 100, height = 50))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use the data from the above folder and try to do inference using the image_client thats included with the container. For each image we will try to get 3 different classifications from the model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!python3 image_client.py -u localhost:8000 -m inception_graphdef -x 1 -s INCEPTION -c 3 /tritonworkspace/images"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now lets try to batch all the four data into a single request. We can see all the 4 image data was sent to the server in one single HTTP request. This will be the first example of how batching works in  Triton."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!python3 image_client.py -u localhost:8000 -m inception_graphdef -x 1 -s INCEPTION -c 3 /tritonworkspace/images -b 4"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Determine throughput and latency with Perf Analyzer <a class=\"anchor\" id=\"PerfAnalyzer\"></a>\n",
    "\n",
    "Once the model is deployed for inference in Triton, we can measure its inference performance using `perf_analyzer`. The perf_analyzer application generates inference requests to the deployed model and measures the throughput and latency of those requests. For more information on `perf_analyzer` utility, please refer this [link](https://github.com/triton-inference-server/server/blob/main/docs/perf_analyzer.md) "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now change the config of the mode from GPU to CPU in the triton model config.pbtxt. We don't need to reload the triton server here as it will pull the latest config.pbtxt automatically.\n",
    "\n",
    "```\n",
    "instance_group [\n",
    "   {\n",
    "     count: 1\n",
    "     kind: KIND_CPU\n",
    "   }\n",
    "]\n",
    "```"
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!perf_analyzer -u localhost:8000  -m inception_graphdef --percentile=95 --concurrency-range=4 -b 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now change the config and move the model to GPU\n",
    "\n",
    "```\n",
    "instance_group [\n",
    "   {\n",
    "     count: 1\n",
    "     kind: KIND_GPU\n",
    "   }\n",
    "]\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!perf_analyzer -u localhost:8000  -m inception_graphdef --percentile=95 --concurrency-range=4 -b 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now change the config to have 2 instances of the same model\n",
    "\n",
    "```\n",
    "instance_group [\n",
    "   {\n",
    "     count: 2\n",
    "     kind: KIND_GPU\n",
    "   }\n",
    "]\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!perf_analyzer -u localhost:8000  -m inception_graphdef --percentile=95 --concurrency-range=4 -b 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Now add the dynamic bathing block to the config of the model and send request with bs=2 and bs=4 and show the queue delay.\n",
    "\n",
    "```\n",
    "dynamic_batching {\n",
    "   preferred_batch_size: [ 4, 8 ]\n",
    "   max_queue_delay_microseconds: 2000\n",
    "}\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Now use the perf analyzer for measuring the performance again with different batch size and see the queue delay"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!perf_analyzer -u localhost:8000  -m inception_graphdef --percentile=95 --concurrency-range=1:4 -b 4"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now lets add minimum queue delay to see what happens "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!perf_analyzer -u localhost:8000 -m inception_graphdef --percentile=95 --concurrency-range=1:4 -b 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now lets kill the Triton Server using Ctrl + C in the terminal"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}